"""
0x1d å®æ—¶ä¿¡å·è’¸é¦ (Real-time Signal Distillation)
==================================================
ä» monitor_0x1d.py æ”¶é›†çš„ trade_snaps æ•°æ®ä¸­å­¦ä¹  0x1d çš„å…¥åœºæ—¶æœºå’Œæ–¹å‘å†³ç­–,
ç”Ÿæˆä¸€ä¸ªèƒ½åœ¨ BTC ä»»æ„è¡Œæƒ…ä¸‹å®æ—¶è¾“å‡º UP / DOWN / HOLD ä¿¡å·çš„æ¨¡å‹ã€‚

æ ¸å¿ƒæ€è·¯:
  - æ¯ç¬” 0x1d çš„äº¤æ˜“ = ä¸€ä¸ªã€Œåœ¨å½“å‰å¸‚åœºçŠ¶æ€ä¸‹çš„å…¥åœºå†³ç­–ã€æ ·æœ¬
  - ç‰¹å¾ = çº¯å¸‚åœºçŠ¶æ€ (BTC åŠ¨é‡/æ³¢åŠ¨ç‡/è¶‹åŠ¿ + CL ä»·å·® + PM ç›˜å£)
  - æ ‡ç­¾ = äº¤æ˜“æ–¹å‘ (UP=1, DOWN=0)
  - æ¨¡å‹è¾“å‡ºæ¦‚ç‡ â†’ é«˜ç½®ä¿¡åº¦äº§ç”Ÿä¿¡å·, ä½ç½®ä¿¡åº¦ = HOLD

ä¸æ—§ç‰ˆåŒºåˆ«:
  - æ—§ç‰ˆ: é¢„æµ‹æ•´ä¸ª 5min çª—å£ UP/DOWN æ–¹å‘ (76ä¸ªæ ·æœ¬, æ— æ³•å­¦ä¼š)
  - æ–°ç‰ˆ: å­¦ä¹ æ¯ä¸€ç¬”äº¤æ˜“æ—¶åˆ»çš„å¸‚åœºç‰¹å¾ â†’ æ–¹å‘ (æ•°åƒæ ·æœ¬, å®æ—¶å¯ç”¨)
  - äº§å‡º: ä¸€ä¸ªå®æ—¶ä¿¡å·ç”Ÿæˆå™¨, è¾“å…¥å½“å‰å¸‚åœºæ•°æ® â†’ è¾“å‡ºäº¤æ˜“ä¿¡å·

ç”¨æ³•:
  python scripts/distill_signal.py                # å®Œæ•´è®­ç»ƒ + è¯„ä¼°
  python scripts/distill_signal.py --report       # åªçœ‹æ•°æ®åˆ†æ
  python scripts/distill_signal.py --threshold 0.65  # è‡ªå®šä¹‰ä¿¡å·é˜ˆå€¼
  python scripts/distill_signal.py --rich-only    # ä»…ç”¨æœ‰ref_tsçš„é«˜è´¨é‡æ•°æ®
  python scripts/distill_signal.py --pure-market  # ä»…ç”¨çº¯å¸‚åœºç‰¹å¾ (æ’é™¤æŒä»“/è¡Œä¸º)
  python scripts/distill_signal.py --compare      # å¯¹æ¯”å…¨ç‰¹å¾ vs çº¯å¸‚åœºç‰¹å¾
  python scripts/distill_signal.py --outcome      # â­ ç”¨çª—å£ç»“ç®—ç»“æœä½œä¸ºè®­ç»ƒæ ‡ç­¾
                                                   #   (é¢„æµ‹å“ªè¾¹èµ¢, è€Œéæ¨¡ä»¿0x1d)
  python scripts/distill_signal.py --outcome --pure-market  # æ¨è: çº¯å¸‚åœº+ç»“ç®—æ ‡ç­¾

è¯„ä¼°æŒ‡æ ‡:
  æ¨¡å‹è¯„ä¼°ä»¥ **çª—å£PnLæ¨¡æ‹Ÿ** ä¸ºæ ¸å¿ƒ: æŒ‰ä¿¡å·ä¸‹æ³¨ â†’ çª—å£ç»“ç®— â†’ è®¡ç®—çœŸå®ç›ˆäº
  è€Œéç®€å•çš„æ–¹å‘å‡†ç¡®ç‡ã€‚

äº§å‡º:
  data/distill_models/signal_model.pkl     â€” LightGBM æ¨¡å‹
  data/distill_models/signal_config.json   â€” ç‰¹å¾åˆ—è¡¨ + ä¿¡å·é˜ˆå€¼ + è®­ç»ƒæ¨¡å¼
"""

import sqlite3
import json
import sys
import pickle
import warnings
from pathlib import Path
from datetime import datetime, timezone
from collections import defaultdict

# Windows GBK ç¼–ç å…¼å®¹
if sys.platform == "win32":
    try:
        sys.stdout.reconfigure(encoding="utf-8")
    except Exception:
        pass

import numpy as np
import pandas as pd
from sklearn.model_selection import GroupKFold
from sklearn.metrics import (
    accuracy_score, roc_auc_score, f1_score,
    classification_report, confusion_matrix,
)
import lightgbm as lgb

warnings.filterwarnings("ignore")

# â”€â”€ è·¯å¾„ â”€â”€
ROOT = Path(__file__).resolve().parent.parent
DB_PATH = ROOT / "data" / "0x1d_data.db"
MODEL_DIR = ROOT / "data" / "distill_models"
MODEL_DIR.mkdir(parents=True, exist_ok=True)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç‰¹å¾å®šä¹‰: ä»…ä½¿ç”¨çº¯å¸‚åœºçŠ¶æ€ç‰¹å¾
# ä¸åŒ…å«æŒä»“ã€çª—å£ç´¯è®¡ç­‰æ¨ç†æ—¶ä¸å­˜åœ¨çš„å­—æ®µ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Binance BTC åŠ¨é‡ (å¤šæ—¶é—´å°ºåº¦)
BN_MOMENTUM = [
    "bn_mom_1s", "bn_mom_3s", "bn_mom_5s", "bn_mom_10s", "bn_mom_15s",
    "bn_mom_30s", "bn_mom_60s", "bn_mom_120s",
]
# Binance BTC æ³¢åŠ¨ç‡
BN_VOLATILITY = ["bn_vol_10s", "bn_vol_30s", "bn_vol_60s"]
# Binance é«˜çº§æŒ‡æ ‡ (è¶‹åŠ¿/z-score/ç™¾åˆ†ä½)
BN_ADVANCED = [
    "bn_mom_accel_5s", "bn_mom_accel_10s",
    "bn_trend_30s", "bn_trend_60s",
    "bn_mom_z_5s", "bn_pctl_60s",
]
# Chainlink RTDS åŠ¨é‡
CL_MOMENTUM = [
    "mom_1s", "mom_3s", "mom_5s", "mom_10s", "mom_15s",
    "mom_30s", "mom_60s", "mom_120s",
]
# Chainlink æ³¢åŠ¨ç‡
CL_VOLATILITY = ["btc_vol_10s", "btc_vol_30s", "btc_vol_60s"]
# Chainlink é«˜çº§æŒ‡æ ‡
CL_ADVANCED = [
    "mom_accel_5s", "mom_accel_10s",
    "cl_trend_30s", "cl_trend_60s", "cl_trend_120s",
    "cl_mom_z_5s", "cl_mom_z_10s",
    "cl_pctl_60s", "cl_pctl_300s",
    "cl_dir_changes_30s", "cl_dir_changes_60s",
]
# è·¨æ•°æ®æºç‰¹å¾ (BN vs CL å·®å¼‚)
CROSS_SOURCE = [
    "cl_bn_spread", "cl_bn_mom_diff_5s", "cl_bn_mom_diff_10s",
    "cl_bn_trend_agree",
    "btc_delta_pct", "btc_delta_ptb",
    "bn_delta_pct", "bn_delta_ptb",
    "btc_above_ptb",
]
# PM ç›˜å£ç‰¹å¾ (ä»·æ ¼æœºä¼šæŒ‡æ ‡)
PM_ORDERBOOK = [
    "up_bid", "up_ask", "dn_bid", "dn_ask",
    "up_ba_spread", "dn_ba_spread",
    "pm_spread", "pm_edge",
]
# çª—å£ä¸Šä¸‹æ–‡ (æ¨ç†æ—¶å¯è·å¾—)
WINDOW_CONTEXT = ["elapsed", "elapsed_pct"]
# æ—¶é—´ç‰¹å¾ (äº¤æ˜“æ—¶æ®µå¯¹è¡Œä¸ºæ¨¡å¼æœ‰æ˜¾è‘—å½±å“)
TIME_FEATURES = [
    "hour_utc",        # UTC å°æ—¶ (0-23)
    "minute_utc",      # UTC åˆ†é’Ÿ (0-59)
    "day_of_week",     # æ˜ŸæœŸå‡  (0=Mon, 6=Sun)
    "us_session",      # ç¾è‚¡æ—¶æ®µ: 0=closed, 1=pre, 2=regular, 3=after
    "asia_session",    # äºšç›˜ (UTC 0-8)
    "euro_session",    # æ¬§ç›˜ (UTC 7-16)
]
# æŒä»“/è¡Œä¸ºç‰¹å¾ (æ¨ç†æ—¶ bot è‡ªèº«å¯è¿½è¸ª)
POSITION_BEHAVIOR = [
    "net_shares",          # UP-DN åŸå§‹ shares å·®å€¼ (ç»å¯¹é‡, ä¸å½’ä¸€åŒ–)
    "cum_trades",          # å½“å‰çª—å£ç´¯è®¡äº¤æ˜“ç¬”æ•°
    "cum_up_shares",       # UP ç´¯è®¡ shares
    "cum_dn_shares",       # DOWN ç´¯è®¡ shares
    "cum_up_cost",         # UP ç´¯è®¡æˆæœ¬ USDC
    "cum_dn_cost",         # DOWN ç´¯è®¡æˆæœ¬ USDC
    "avg_up_price",        # UP å¹³å‡æˆäº¤ä»·
    "avg_dn_price",        # DOWN å¹³å‡æˆäº¤ä»·
    "pos_imbalance",       # ä»“ä½åå·® (UP-DN)/max(UP,DN)
    "same_side_streak",    # è¿ç»­åŒå‘ç¬”æ•° (æ­£=UP, è´Ÿ=DN)
    "trade_velocity",      # äº¤æ˜“é€Ÿåº¦ (ç¬”/çª—å£è¿›åº¦)
    "time_since_last",     # è·ä¸Šç¬”äº¤æ˜“ç§’æ•°
    "burst_seq",           # çªå‘åºå· (1så†…è¿ç»­äº¤æ˜“)
    "is_burst",            # æ˜¯å¦çªå‘äº¤æ˜“
    "up_price_vs_fair",    # UP ä»·æ ¼ vs å…¬å¹³ä»·åå·®
    "dn_price_vs_fair",    # DN ä»·æ ¼ vs å…¬å¹³ä»·åå·®
]
# Burst èšåˆç‰¹å¾ (å»é‡æ—¶ç”±å¤šç¬”äº¤æ˜“èšåˆç”Ÿæˆ, åæ˜ å•æ¬¡å†³ç­–çš„æ‰«å•åŠ›åº¦)
BURST_AGGREGATE = [
    "burst_n_fills",       # æœ¬æ¬¡å†³ç­–çš„æˆäº¤ç¬”æ•° (1=å•æ¡£, >1=æ‰«å¤šæ¡£)
    "burst_total_shares",  # æœ¬æ¬¡å†³ç­–æ€» shares
    "burst_total_usdc",    # æœ¬æ¬¡å†³ç­–æ€» USDC
    "burst_avg_price",     # æœ¬æ¬¡å†³ç­–åŠ æƒå‡ä»· (usdc/shares)
]

# å…¨éƒ¨ä¿¡å·ç‰¹å¾ (å«æŒä»“/è¡Œä¸ºç‰¹å¾)
SIGNAL_FEATURES = (
    BN_MOMENTUM + BN_VOLATILITY + BN_ADVANCED +
    CL_MOMENTUM + CL_VOLATILITY + CL_ADVANCED +
    CROSS_SOURCE + PM_ORDERBOOK + WINDOW_CONTEXT + TIME_FEATURES +
    POSITION_BEHAVIOR + BURST_AGGREGATE
)

# çº¯å¸‚åœºç‰¹å¾ (æ’é™¤æŒä»“/è¡Œä¸º/burst â€” é¿å…è‡ªç›¸å…³æ³„æ¼)
# è¿™äº›ç‰¹å¾ä»…ä¾èµ–å¤–éƒ¨å¸‚åœºæ•°æ®, ä¸ä¾èµ– 0x1d è‡ªèº«çš„äº¤æ˜“è¡Œä¸º
PURE_MARKET_FEATURES = (
    BN_MOMENTUM + BN_VOLATILITY + BN_ADVANCED +
    CL_MOMENTUM + CL_VOLATILITY + CL_ADVANCED +
    CROSS_SOURCE + PM_ORDERBOOK + WINDOW_CONTEXT + TIME_FEATURES
)

# æ’é™¤çš„ç‰¹å¾ (çº¯å…ƒæ•°æ®/æ ‡ç­¾ â€” æ¨ç†æ—¶æ— æ„ä¹‰)
EXCLUDED = [
    "side", "shares", "price",     # æ ‡ç­¾å’Œæˆäº¤ç»†èŠ‚
    "ref_ts", "feature_latency", "ts_source",  # è°ƒè¯•å…ƒæ•°æ®
]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. æ•°æ®åŠ è½½
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def load_settlement_labels() -> dict:
    """åŠ è½½ç»“ç®—æ•°æ®, è¿”å› {slug: {won, pnl}} æ˜ å°„, ç”¨äº outcome æ ‡ç­¾å’Œ PnL æ¨¡æ‹Ÿ"""
    conn = sqlite3.connect(str(DB_PATH))
    rows = conn.execute(
        "SELECT slug, pnl, won, up_cost, dn_cost, up_shares, dn_shares "
        "FROM settlements ORDER BY settled_at"
    ).fetchall()
    conn.close()
    settle = {}
    for slug, pnl, won, u_cost, d_cost, u_sh, d_sh in rows:
        settle[slug] = {
            "pnl": pnl, "won": won,
            "up_cost": u_cost or 0, "dn_cost": d_cost or 0,
            "up_shares": u_sh or 0, "dn_shares": d_sh or 0,
        }
    print(f"åŠ è½½ {len(settle)} æ¡ç»“ç®—è®°å½•")
    return settle


def load_trades(rich_only: bool = False) -> pd.DataFrame:
    """åŠ è½½ trade_snaps, è§£æ features JSON"""
    conn = sqlite3.connect(str(DB_PATH))
    rows = conn.execute(
        "SELECT ts, slug, side, shares, price, usdc, features "
        "FROM trade_snaps ORDER BY ts"
    ).fetchall()
    conn.close()

    records = []
    for ts, slug, side, shares, price, usdc, feat_json in rows:
        feat = json.loads(feat_json) if feat_json else {}
        rec = {
            "ts": ts, "slug": slug, "side": side,
            "shares": shares, "price": price, "usdc": usdc,
            "has_ref_ts": "ref_ts" in feat,
        }
        for f in SIGNAL_FEATURES:
            rec[f] = feat.get(f, np.nan)
        records.append(rec)

    df = pd.DataFrame(records)
    if rich_only:
        df = df[df.has_ref_ts].reset_index(drop=True)

    n_up = (df.side == "UP").sum()
    n_dn = (df.side == "DOWN").sum()
    n_ref = df.has_ref_ts.sum()
    n_win = df.slug.nunique()
    print(f"åŠ è½½ {len(df)} ç¬”äº¤æ˜“ ({n_win} ä¸ªçª—å£)")
    print(f"  UP: {n_up}  |  DOWN: {n_dn}  |  æœ‰ref_ts: {n_ref}")
    return df


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. çªå‘èšç±» (Burst Deduplication)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def deduplicate_bursts(df: pd.DataFrame, gap_sec: int = 3) -> pd.DataFrame:
    """
    å°†è¿ç»­çš„åŒæ–¹å‘äº¤æ˜“èšåˆä¸ºä¸€ä¸ªã€Œå†³ç­–ç‚¹ã€:
    - åŒçª—å£ + åŒæ–¹å‘ + æ—¶é—´é—´éš” < gap_sec â†’ è§†ä¸ºåŒä¸€æ¬¡å…¥åœºå†³ç­–
    - å¸‚åœºç‰¹å¾: å–é¦–ç¬” (å†³ç­–ç¬é—´çš„å¸‚åœºå¿«ç…§)
    - æŒä»“ç‰¹å¾: å–æœ«ç¬” (æ‰«å•å®Œæˆåçš„æŒä»“çŠ¶æ€)
    - æ–°å¢ burst èšåˆç‰¹å¾: ç¬”æ•°/æ€»shares/æ€»usdc/å‡ä»· (åæ˜ æ‰«å•åŠ›åº¦)

    æ ¸å¿ƒé€»è¾‘: 0x1d ä¸€æ¬¡ä¸‹å•å¯èƒ½æ‰«å¤šä¸ªæ¡£å£ â†’ CLOB è®°å½•å¤šç¬” fill,
    ä½†è¿™æ˜¯ä¸€ä¸ªä¿¡å·, ä¸æ˜¯å¤šä¸ªä¿¡å·ã€‚æ¨¡å‹éœ€è¦æ„ŸçŸ¥ã€Œå•æ¡£ vs å¤šæ¡£æ‰«å•ã€ã€‚
    """
    df = df.sort_values(["slug", "side", "ts"]).reset_index(drop=True)

    # æ ‡è®° burst è¾¹ç•Œ
    new_burst = (
        (df["slug"] != df["slug"].shift()) |
        (df["side"] != df["side"].shift()) |
        ((df["ts"] - df["ts"].shift()) > gap_sec)
    )
    df["burst_id"] = new_burst.cumsum()

    # â”€â”€ åŸºç¡€èšåˆ â”€â”€
    agg = df.groupby("burst_id").agg(
        ts=("ts", "first"),
        slug=("slug", "first"),
        side=("side", "first"),
        shares=("shares", "sum"),
        usdc=("usdc", "sum"),
        n_trades=("ts", "count"),
        has_ref_ts=("has_ref_ts", "first"),
    ).reset_index(drop=True)

    # â”€â”€ å¸‚åœºç‰¹å¾: å–é¦–ç¬”äº¤æ˜“ (å†³ç­–ç¬é—´çš„å¸‚åœºå¿«ç…§) â”€â”€
    # è¿™äº›ç‰¹å¾åœ¨å†³ç­–é‚£ä¸€åˆ»å°±å·²ç¡®å®š, ä¸å—åç»­ fill å½±å“
    MARKET_FEATURES = (
        BN_MOMENTUM + BN_VOLATILITY + BN_ADVANCED +
        CL_MOMENTUM + CL_VOLATILITY + CL_ADVANCED +
        CROSS_SOURCE + PM_ORDERBOOK + WINDOW_CONTEXT + TIME_FEATURES
    )
    first_idx = df.groupby("burst_id").head(1).index
    market_df = df.loc[first_idx, [f for f in MARKET_FEATURES if f in df.columns]].reset_index(drop=True)

    # â”€â”€ æŒä»“ç‰¹å¾: å–æœ«ç¬”äº¤æ˜“ (æ‰«å•å®Œæˆåçš„æŒä»“çŠ¶æ€) â”€â”€
    # æ¨ç†æ—¶ bot ä¹Ÿæ˜¯ç­‰ burst ç»“æŸåæ‰çŸ¥é“æœ€ç»ˆæŒä»“
    last_idx = df.groupby("burst_id").tail(1).index
    pos_cols = [f for f in POSITION_BEHAVIOR if f in df.columns]
    pos_df = df.loc[last_idx, pos_cols].reset_index(drop=True)

    # â”€â”€ Burst èšåˆç‰¹å¾ (æ–°å¢: åæ˜ æ‰«å•å¼ºåº¦) â”€â”€
    burst_agg = df.groupby("burst_id").agg(
        burst_n_fills=("ts", "count"),                      # æ‰«äº†å‡ ä¸ªæ¡£
        burst_total_shares=("shares", "sum"),                # æ€» shares
        burst_total_usdc=("usdc", "sum"),                    # æ€» USDC
    ).reset_index(drop=True)
    # åŠ æƒå‡ä»· = usdc / shares
    burst_agg["burst_avg_price"] = (
        burst_agg["burst_total_usdc"] / burst_agg["burst_total_shares"].replace(0, np.nan)
    ).round(4)

    result = pd.concat([agg, market_df, pos_df, burst_agg], axis=1)

    ratio = len(df) / len(result) if len(result) > 0 else 0
    multi_fill = (result.burst_n_fills > 1).sum()
    print(f"\nçªå‘èšç±»: {len(df)} ç¬”äº¤æ˜“ â†’ {len(result)} ä¸ªå†³ç­–ç‚¹ (å‹ç¼© {ratio:.1f}x)")
    print(f"  å•æ¡£æˆäº¤: {len(result) - multi_fill} | å¤šæ¡£æ‰«å•: {multi_fill} ({multi_fill/len(result)*100:.1f}%)")
    print(f"  å¹³å‡ burst å¤§å°: {result.burst_n_fills.mean():.1f} ç¬”, ${result.burst_total_usdc.mean():.1f}")
    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. ç‰¹å¾åˆ†æ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def analyze_features(df: pd.DataFrame) -> list:
    """åˆ†ææ¯ä¸ªç‰¹å¾å¯¹ UP/DOWN çš„åŒºåˆ†èƒ½åŠ›"""
    print("\n" + "=" * 72)
    print("  ç‰¹å¾åˆ†æ: UP vs DOWN åŒºåˆ†åº¦ (Cohen's d)")
    print("=" * 72)

    up = df[df.side == "UP"]
    dn = df[df.side == "DOWN"]

    results = []
    for f in SIGNAL_FEATURES:
        u_vals = up[f].dropna()
        d_vals = dn[f].dropna()
        if len(u_vals) < 10 or len(d_vals) < 10:
            continue
        mu, md = u_vals.mean(), d_vals.mean()
        pooled = np.sqrt((u_vals.std() ** 2 + d_vals.std() ** 2) / 2)
        d = abs(mu - md) / pooled if pooled > 0 else 0
        direction = "â†‘UP" if mu > md else "â†“DN"
        results.append({
            "feature": f, "mean_up": mu, "mean_dn": md,
            "cohens_d": d, "direction": direction,
            "coverage": df[f].notna().mean() * 100,
        })

    results.sort(key=lambda x: x["cohens_d"], reverse=True)

    print(f"\n{'ç‰¹å¾':<32} {'UPå‡å€¼':>10} {'DNå‡å€¼':>10} {'Cohen d':>8} {'æ–¹å‘':>5} {'è¦†ç›–':>6}")
    print("-" * 75)
    for r in results[:30]:
        bar = "â–ˆ" * int(r["cohens_d"] / max(x["cohens_d"] for x in results) * 15)
        print(
            f"  {r['feature']:<30} {r['mean_up']:10.4f} {r['mean_dn']:10.4f} "
            f"{r['cohens_d']:8.4f} {r['direction']:>5} {r['coverage']:5.1f}% {bar}"
        )
    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. æ¨¡å‹è®­ç»ƒ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def train_signal_model(df: pd.DataFrame, threshold: float = 0.60,
                       feature_set: list | None = None,
                       label: str = "å…¨ç‰¹å¾",
                       outcome_labels: dict | None = None):
    """
    è®­ç»ƒå®æ—¶å…¥åœºä¿¡å·æ¨¡å‹:
      - è¾“å…¥: å¸‚åœºçŠ¶æ€ç‰¹å¾ (å¯é€‰æ’é™¤æŒä»“ç‰¹å¾)
      - è¾“å‡º: P(UP), ç”¨é˜ˆå€¼åˆ†ä¸º UP / DOWN / HOLD
      - éªŒè¯: GroupKFold (æŒ‰çª—å£åˆ†ç»„, æ— æ—¶é—´æ³„æ¼)
      - outcome_labels: è‹¥æä¾›, ä»¥çª—å£ç»“ç®—èµ¢æ–¹ä½œä¸ºè®­ç»ƒæ ‡ç­¾ (è€Œéæ¨¡ä»¿ 0x1d)
    """
    features = feature_set or SIGNAL_FEATURES
    training_mode = "outcome" if outcome_labels else "mimicry"
    print("\n" + "=" * 72)
    print(f"  è®­ç»ƒ: å®æ—¶å…¥åœºä¿¡å·æ¨¡å‹ [{label}]")
    print(f"  ç‰¹å¾: {len(features)} | è®­ç»ƒæ ‡ç­¾: {'çª—å£ç»“ç®—ç»“æœ' if outcome_labels else 'æ¨¡ä»¿0x1dæ–¹å‘'}")
    print("=" * 72)

    X = df[features].copy()
    groups = df["slug"]                         # æŒ‰çª—å£åˆ†ç»„

    if outcome_labels:
        # â”€â”€ ç»“ç®—æ ‡ç­¾: 1=UPèµ¢, 0=DOWNèµ¢ â”€â”€
        df = df.copy()
        df["_outcome"] = df["slug"].map(
            lambda s: 1 if outcome_labels.get(s, {}).get("won") == "UP"
            else (0 if outcome_labels.get(s, {}).get("won") == "DOWN" else np.nan)
        )
        valid_mask = df["_outcome"].notna()
        if valid_mask.sum() < len(df):
            n_drop = len(df) - valid_mask.sum()
            print(f"  æ’é™¤ {n_drop} ç¬”æ— ç»“ç®—æ•°æ®çš„äº¤æ˜“ ({n_drop/len(df)*100:.1f}%)")
            X = X[valid_mask].reset_index(drop=True)
            groups = groups[valid_mask].reset_index(drop=True)
            df = df[valid_mask].reset_index(drop=True)
        y = df["_outcome"].astype(int)
        n_up_win = (y == 1).sum()
        n_dn_win = (y == 0).sum()
        print(f"  æ ‡ç­¾åˆ†å¸ƒ: UPèµ¢={n_up_win} ({n_up_win/len(y)*100:.1f}%) | DOWNèµ¢={n_dn_win} ({n_dn_win/len(y)*100:.1f}%)")
    else:
        y = (df["side"] == "UP").astype(int)       # 1=UP, 0=DOWN

    # ç‰¹å¾è¦†ç›–ç‡
    coverage = X.notna().mean().sort_values()
    low = (coverage < 0.3).sum()
    print(f"\nç‰¹å¾æ•°: {len(SIGNAL_FEATURES)}, è¦†ç›–ç‡ <30% çš„ç‰¹å¾: {low}")
    if low > 0:
        print(f"  ä½è¦†ç›–ç‰¹å¾: {coverage[coverage < 0.3].index.tolist()}")

    # â”€â”€ LightGBM å‚æ•° â”€â”€
    params = dict(
        objective="binary",
        metric="binary_logloss",
        n_estimators=500,
        max_depth=6,
        num_leaves=31,
        learning_rate=0.03,
        subsample=0.8,
        colsample_bytree=0.7,
        min_child_samples=30,
        reg_alpha=0.3,
        reg_lambda=0.3,
        random_state=42,
        verbose=-1,
        n_jobs=-1,
    )

    # â”€â”€ GroupKFold CV â”€â”€
    n_windows = groups.nunique()
    n_splits = min(5, n_windows)
    gkf = GroupKFold(n_splits=n_splits)

    print(f"\n{n_splits}-Fold GroupKFold CV ({n_windows} ä¸ªçª—å£)\n")

    all_probs = np.full(len(df), np.nan)
    fold_metrics = []

    for fold, (tr_idx, te_idx) in enumerate(gkf.split(X, y, groups)):
        X_tr, X_te = X.iloc[tr_idx], X.iloc[te_idx]
        y_tr, y_te = y.iloc[tr_idx], y.iloc[te_idx]

        model = lgb.LGBMClassifier(**params)
        model.fit(
            X_tr, y_tr,
            eval_set=[(X_te, y_te)],
            callbacks=[lgb.early_stopping(50, verbose=False)],
        )

        probs = model.predict_proba(X_te)[:, 1]
        all_probs[te_idx] = probs
        preds = (probs > 0.5).astype(int)

        acc = accuracy_score(y_te, preds)
        try:
            auc = roc_auc_score(y_te, probs)
        except ValueError:
            auc = 0.5
        f1 = f1_score(y_te, preds, average="macro")
        n_win = groups.iloc[te_idx].nunique()

        fold_metrics.append({"acc": acc, "auc": auc, "f1": f1, "n": len(te_idx), "w": n_win})
        print(f"  Fold {fold+1}: Acc={acc:.4f}  AUC={auc:.4f}  F1={f1:.4f}  "
              f"(samples={len(te_idx)}, windows={n_win})")

    cv_acc = np.mean([m["acc"] for m in fold_metrics])
    cv_auc = np.mean([m["auc"] for m in fold_metrics])
    cv_f1 = np.mean([m["f1"] for m in fold_metrics])
    acc_std = np.std([m["acc"] for m in fold_metrics])
    auc_std = np.std([m["auc"] for m in fold_metrics])
    print(f"\n  CV å¹³å‡: Acc={cv_acc:.4f}Â±{acc_std:.4f}  AUC={cv_auc:.4f}Â±{auc_std:.4f}  F1={cv_f1:.4f}")
    print(f"  åŸºå‡†çº¿ (éšæœº): Acc=0.5000  AUC=0.5000")
    lift = (cv_acc - 0.5) / 0.5 * 100
    print(f"  æå‡: +{lift:.1f}% over random")

    # â”€â”€ ä¸åŒé˜ˆå€¼çš„ä¿¡å·åˆ†æ â”€â”€
    print(f"\n{'é˜ˆå€¼':<8} {'ä¿¡å·ç‡':>8} {'UP%':>7} {'DN%':>7} {'HOLD%':>7} {'ä¿¡å·Acc':>8} {'ä¿¡å·F1':>8}")
    print("-" * 58)

    valid = ~np.isnan(all_probs)
    probs_v = all_probs[valid]
    y_v = y.values[valid]

    best_t, best_metric = threshold, 0
    for t in [0.50, 0.52, 0.55, 0.58, 0.60, 0.62, 0.65, 0.70, 0.75]:
        sig_up = probs_v > t
        sig_dn = probs_v < (1 - t)
        sig = sig_up | sig_dn

        if sig.sum() > 0:
            sig_preds = np.where(sig_up[sig], 1, 0)
            sig_acc = accuracy_score(y_v[sig], sig_preds)
            sig_f1 = f1_score(y_v[sig], sig_preds, average="macro")
        else:
            sig_acc = sig_f1 = 0

        pct_up = sig_up.mean() * 100
        pct_dn = sig_dn.mean() * 100
        pct_hold = (1 - sig.mean()) * 100

        # ç»¼åˆå¾—åˆ†: ä¿¡å·å‡†ç¡®ç‡ Ã— ä¿¡å·è¦†ç›–ç‡çš„å¹³æ–¹æ ¹
        score = sig_acc * np.sqrt(sig.mean()) if sig.mean() > 0 else 0
        if score > best_metric:
            best_metric = score
            best_t = t

        marker = " <-- best" if abs(t - best_t) < 0.005 and t == best_t else ""
        print(f"  {t:.2f}   {sig.mean()*100:7.1f}% {pct_up:6.1f}% {pct_dn:6.1f}% "
              f"{pct_hold:6.1f}% {sig_acc:8.4f} {sig_f1:8.4f}{marker}")

    # â”€â”€ è®­ç»ƒæœ€ç»ˆæ¨¡å‹ â”€â”€
    print(f"\nè®­ç»ƒæœ€ç»ˆæ¨¡å‹ (å…¨é‡æ•°æ®, {len(df)} samples)...")
    final_model = lgb.LGBMClassifier(**params)
    final_model.fit(X, y)

    # â”€â”€ ç‰¹å¾é‡è¦æ€§ â”€â”€
    imp = pd.DataFrame({
        "feature": features,
        "importance": final_model.feature_importances_,
    }).sort_values("importance", ascending=False)

    print(f"\nTop 25 ç‰¹å¾é‡è¦æ€§:")
    print(f"{'ç‰¹å¾':<34} {'é‡è¦æ€§':>8}")
    print("-" * 44)
    max_imp = imp["importance"].max()
    for _, row in imp.head(25).iterrows():
        bar = "â–ˆ" * int(row["importance"] / max_imp * 20) if max_imp > 0 else ""
        print(f"  {row['feature']:<32} {row['importance']:>6.0f} {bar}")

    # â”€â”€ éªŒè¯: æ¨¡å‹å­¦åˆ°çš„æ–¹å‘æ˜¯å¦æ­£ç¡®? â”€â”€
    print(f"\néªŒè¯: æ¨¡å‹é¢„æµ‹ vs BTC åŠ¨é‡æ–¹å‘")
    _verify_direction_logic(df, all_probs)

    return final_model, {
        "cv_acc": cv_acc,
        "cv_auc": cv_auc,
        "cv_f1": cv_f1,
        "best_threshold": best_t,
        "fold_metrics": fold_metrics,
        "all_probs": all_probs,
        "feature_importance": imp,
        "features_used": features,
        "label": label,
        "training_mode": training_mode,
        "df_used": df,  # è®­ç»ƒæ—¶å®é™…ä½¿ç”¨çš„ df (å¯èƒ½å·²è¿‡æ»¤)
    }


def _verify_direction_logic(df: pd.DataFrame, probs: np.ndarray):
    """éªŒè¯æ¨¡å‹æ˜¯å¦å­¦åˆ°äº†æ­£ç¡®çš„è¶‹åŠ¿è·Ÿè¸ªé€»è¾‘"""
    valid = ~np.isnan(probs) & df["bn_mom_5s"].notna()
    if valid.sum() < 100:
        print("  (æ•°æ®ä¸è¶³, è·³è¿‡)")
        return

    mom = df.loc[valid, "bn_mom_5s"].values
    p = probs[valid]
    y_true = (df.loc[valid, "side"] == "UP").astype(int).values

    # BTC ä¸Šæ¶¨æ—¶ vs ä¸‹è·Œæ—¶
    up_mask = mom > 0.5    # BTC æ˜æ˜¾ä¸Šæ¶¨
    dn_mask = mom < -0.5   # BTC æ˜æ˜¾ä¸‹è·Œ

    if up_mask.sum() > 10 and dn_mask.sum() > 10:
        p_up_when_btc_up = p[up_mask].mean()
        p_up_when_btc_dn = p[dn_mask].mean()
        actual_up_when_btc_up = y_true[up_mask].mean()
        actual_up_when_btc_dn = y_true[dn_mask].mean()

        print(f"  BTCæ¶¨ (mom>0.5, n={up_mask.sum()}):")
        print(f"    æ¨¡å‹ P(UP) = {p_up_when_btc_up:.3f}  |  å®é™… UP% = {actual_up_when_btc_up:.3f}")
        print(f"  BTCè·Œ (mom<-0.5, n={dn_mask.sum()}):")
        print(f"    æ¨¡å‹ P(UP) = {p_up_when_btc_dn:.3f}  |  å®é™… UP% = {actual_up_when_btc_dn:.3f}")

        if p_up_when_btc_up > p_up_when_btc_dn:
            print(f"  âœ“ æ¨¡å‹æ­£ç¡®å­¦åˆ°äº†è¶‹åŠ¿è·Ÿè¸ªæ¨¡å¼ (BTCæ¶¨â†’ä¹°UP, BTCè·Œâ†’ä¹°DOWN)")
        else:
            print(f"  âœ— æ¨¡å‹æ–¹å‘å¯èƒ½æœ‰é—®é¢˜")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. ä¿¡å·å›æµ‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def backtest_signals(df: pd.DataFrame, probs: np.ndarray, threshold: float):
    """å›æµ‹ä¿¡å·è´¨é‡: æŒ‰çª—å£åˆ†ææ¨¡å‹ä¿¡å· vs 0x1d å®é™…è¡Œä¸º"""
    print("\n" + "=" * 72)
    print(f"  ä¿¡å·å›æµ‹ (é˜ˆå€¼={threshold:.2f})")
    print("=" * 72)

    valid = ~np.isnan(probs)
    df_v = df[valid].copy()
    p_v = probs[valid]

    df_v["prob_up"] = p_v
    df_v["signal"] = "HOLD"
    df_v.loc[df_v.prob_up > threshold, "signal"] = "UP"
    df_v.loc[df_v.prob_up < (1 - threshold), "signal"] = "DOWN"
    df_v["correct"] = (
        ((df_v.signal == "UP") & (df_v.side == "UP")) |
        ((df_v.signal == "DOWN") & (df_v.side == "DOWN"))
    )

    signaled = df_v[df_v.signal != "HOLD"]
    held = df_v[df_v.signal == "HOLD"]

    print(f"\næ€»å†³ç­–ç‚¹: {len(df_v)}")
    print(f"  äº§ç”Ÿä¿¡å·: {len(signaled)} ({len(signaled)/len(df_v)*100:.1f}%)")
    print(f"  HOLD:     {len(held)} ({len(held)/len(df_v)*100:.1f}%)")

    if len(signaled) > 0:
        sig_acc = signaled.correct.mean()
        print(f"\nä¿¡å·å‡†ç¡®ç‡: {sig_acc:.4f} ({signaled.correct.sum()}/{len(signaled)})")

        # æŒ‰ä¿¡å·æ–¹å‘åˆ†æ
        for sig_dir in ["UP", "DOWN"]:
            sub = signaled[signaled.signal == sig_dir]
            if len(sub) > 0:
                acc = sub.correct.mean()
                avg_conf = sub.prob_up.apply(lambda p: max(p, 1-p)).mean()
                print(f"  {sig_dir} ä¿¡å·: {len(sub)} ç¬”, Acc={acc:.4f}, å¹³å‡ç½®ä¿¡åº¦={avg_conf:.3f}")

    # æŒ‰çª—å£åˆ†æ
    print(f"\næŒ‰çª—å£åˆ†æ:")
    window_stats = []
    for slug, grp in df_v.groupby("slug"):
        sig = grp[grp.signal != "HOLD"]
        if len(sig) == 0:
            continue
        window_stats.append({
            "slug": slug,
            "total": len(grp),
            "signaled": len(sig),
            "rate": len(sig) / len(grp) * 100,
            "acc": sig.correct.mean(),
            "up_signals": (sig.signal == "UP").sum(),
            "dn_signals": (sig.signal == "DOWN").sum(),
        })

    if window_stats:
        ws = pd.DataFrame(window_stats)
        print(f"  æœ‰ä¿¡å·çš„çª—å£: {len(ws)}/{df_v.slug.nunique()}")
        print(f"  çª—å£å¹³å‡ä¿¡å·ç‡: {ws.rate.mean():.1f}%")
        print(f"  çª—å£å¹³å‡å‡†ç¡®ç‡: {ws.acc.mean():.4f}")
        print(f"  çª—å£å‡†ç¡®ç‡ä¸­ä½æ•°: {ws.acc.median():.4f}")

        # æ˜¾ç¤ºæœ€å¥½å’Œæœ€å·®çš„çª—å£
        ws_sorted = ws.sort_values("acc", ascending=False)
        print(f"\n  æœ€ä½³çª—å£ (Top 5):")
        for _, row in ws_sorted.head(5).iterrows():
            short = row.slug.split("-")[-1] if "-" in row.slug else row.slug
            print(f"    {short}: Acc={row.acc:.3f}, ä¿¡å·={row.signaled}/{row.total}, "
                  f"UP={row.up_signals} DN={row.dn_signals}")
        print(f"  æœ€å·®çª—å£ (Bottom 5):")
        for _, row in ws_sorted.tail(5).iterrows():
            short = row.slug.split("-")[-1] if "-" in row.slug else row.slug
            print(f"    {short}: Acc={row.acc:.3f}, ä¿¡å·={row.signaled}/{row.total}, "
                  f"UP={row.up_signals} DN={row.dn_signals}")

    # ä¿¡å·æ—¶æœºåˆ†æ
    if "elapsed_pct" in signaled.columns and signaled.elapsed_pct.notna().sum() > 0:
        e = signaled.elapsed_pct.dropna()
        print(f"\nä¿¡å·æ—¶æœº (elapsed %):")
        print(f"  25th: {e.quantile(0.25):.0f}%  |  ä¸­ä½æ•°: {e.median():.0f}%  |  75th: {e.quantile(0.75):.0f}%")

    # USDC åˆ†æ
    sig_usdc = signaled.usdc.sum()
    correct_usdc = signaled[signaled.correct].usdc.sum()
    print(f"\nèµ„é‡‘åˆ†é…:")
    print(f"  ä¿¡å·è¦†ç›– USDC: ${sig_usdc:.0f}")
    print(f"  æ­£ç¡®ä¿¡å· USDC: ${correct_usdc:.0f} ({correct_usdc/sig_usdc*100:.1f}%)")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6. çª—å£ PnL æ¨¡æ‹Ÿ (æ ¸å¿ƒè¯„ä¼°)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def simulate_window_pnl(df: pd.DataFrame, probs: np.ndarray, threshold: float,
                        settle: dict, bet_mode: str = "fixed",
                        base_bet: float = 10.0):
    """
    â­ æ ¸å¿ƒè¯„ä¼°: æŒ‰æ¨¡å‹ä¿¡å·åœ¨æ¯ä¸ª5åˆ†é’Ÿçª—å£å†…ä¸‹æ³¨, ç»“ç®—åè®¡ç®—çœŸå®PnL.

    æµç¨‹:
      1. éå†æ¯ä¸ªçª—å£å†…çš„å†³ç­–ç‚¹
      2. æ¨¡å‹è¾“å‡º UP/DOWN/HOLD â†’ æœ‰ä¿¡å·åˆ™æŒ‰æ–¹å‘ä¸‹æ³¨
      3. çª—å£ç»“ç®—: èµ¢æ–¹ shares Ã— $1 = payout
      4. PnL = payout - total_cost

    bet_mode:
      "fixed"      â€” æ¯æ¬¡ä¿¡å·å›ºå®š base_bet ä¸‹æ³¨ (çº¯æ–¹å‘é¢„æµ‹è¯„ä¼°)
      "confidence" â€” æŒ‰æ¨¡å‹ç½®ä¿¡åº¦ç¼©æ”¾: base_bet Ã— (conf - 0.5) / 0.5
                     ç½®ä¿¡åº¦è¶Šé«˜ä¸‹æ³¨è¶Šå¤§, åˆšè¿‡é˜ˆå€¼åªä¸‹ä¸€ç‚¹ç‚¹
    """
    mode_desc = f"{bet_mode} (${base_bet:.0f}/ç¬”)" if bet_mode == "fixed" else f"{bet_mode} (åŸºç¡€${base_bet:.0f})"
    print("\n" + "=" * 72)
    print(f"  â­ çª—å£ PnL æ¨¡æ‹Ÿ (é˜ˆå€¼={threshold:.2f}, ä¸‹æ³¨={mode_desc})")
    print("=" * 72)

    if not settle:
        print("  æ— ç»“ç®—æ•°æ®, è·³è¿‡")
        return None

    valid = ~np.isnan(probs)
    df_v = df[valid].copy()
    p_v = probs[valid]

    df_v["prob_up"] = p_v
    df_v["signal"] = "HOLD"
    df_v.loc[df_v.prob_up > threshold, "signal"] = "UP"
    df_v.loc[df_v.prob_up < (1 - threshold), "signal"] = "DOWN"

    # â”€â”€ é€çª—å£æ¨¡æ‹Ÿ â”€â”€
    window_results = []
    for slug, grp in df_v.groupby("slug"):
        if slug not in settle:
            continue
        won = settle[slug]["won"]
        if won not in ("UP", "DOWN"):
            continue

        up_shares = 0.0
        dn_shares = 0.0
        up_cost = 0.0
        dn_cost = 0.0
        n_signals = 0

        for _, row in grp.iterrows():
            sig = row["signal"]
            if sig == "HOLD":
                continue

            n_signals += 1
            # ä¸‹æ³¨é‡‘é¢
            prob = row["prob_up"]
            conf = max(prob, 1 - prob)  # ç½®ä¿¡åº¦ [0.5, 1.0]
            if bet_mode == "confidence":
                # ç½®ä¿¡åº¦çº¿æ€§ç¼©æ”¾: conf=é˜ˆå€¼æ—¶â†’å°æ³¨, conf=1.0æ—¶â†’2Ã—base_bet
                # scale èŒƒå›´: [0, 2] (é˜ˆå€¼å¤„â‰ˆ0, æ»¡ç½®ä¿¡åº¦=2x)
                scale = (conf - threshold) / (1.0 - threshold) * 2.0 if conf > threshold else 0
                bet = base_bet * max(scale, 0.1)  # æœ€å°‘ 10% base_bet
            else:
                bet = base_bet

            if sig == "UP":
                price = row.get("up_ask", np.nan)
                if pd.isna(price) or price <= 0 or price >= 1:
                    price = 0.55  # fallback
                shares = bet / price
                up_shares += shares
                up_cost += bet
            else:  # DOWN
                price = row.get("dn_ask", np.nan)
                if pd.isna(price) or price <= 0 or price >= 1:
                    price = 0.55
                shares = bet / price
                dn_shares += shares
                dn_cost += bet

        total_cost = up_cost + dn_cost
        if total_cost == 0:
            continue

        # ç»“ç®—: èµ¢æ–¹ shares å„å…‘ $1
        if won == "UP":
            payout = up_shares * 1.0
        else:  # DOWN
            payout = dn_shares * 1.0

        pnl = payout - total_cost

        window_results.append({
            "slug": slug,
            "won": won,
            "up_shares": round(up_shares, 2),
            "dn_shares": round(dn_shares, 2),
            "up_cost": round(up_cost, 2),
            "dn_cost": round(dn_cost, 2),
            "total_cost": round(total_cost, 2),
            "payout": round(payout, 2),
            "pnl": round(pnl, 2),
            "n_signals": n_signals,
            "roi": pnl / total_cost if total_cost > 0 else 0,
            "0x1d_pnl": settle[slug]["pnl"],
        })

    if not window_results:
        print("  æ— å¯åŒ¹é…çš„çª—å£")
        return None

    wr = pd.DataFrame(window_results)

    # â”€â”€ æ±‡æ€»ç»Ÿè®¡ â”€â”€
    total_pnl = wr.pnl.sum()
    total_cost = wr.total_cost.sum()
    total_payout = wr.payout.sum()
    win_rate = (wr.pnl > 0).mean()
    avg_pnl = wr.pnl.mean()
    median_pnl = wr.pnl.median()
    total_roi = total_pnl / total_cost if total_cost > 0 else 0
    actual_total_pnl = wr["0x1d_pnl"].sum()

    print(f"\n  æ¨¡æ‹Ÿçª—å£æ•°: {len(wr)}")
    print(f"  {'â”€' * 50}")
    print(f"  æ€»æŠ•å…¥:       ${total_cost:>12,.2f}")
    print(f"  æ€»å›æ”¶:       ${total_payout:>12,.2f}")
    print(f"  æ€» PnL:       ${total_pnl:>12,.2f}  (ROI: {total_roi:>+.2%})")
    print(f"  {'â”€' * 50}")
    print(f"  çª—å£èƒœç‡:     {win_rate:.2%} ({(wr.pnl > 0).sum()}/{len(wr)})")
    print(f"  å¹³å‡ PnL:     ${avg_pnl:>+.2f}/çª—å£")
    print(f"  ä¸­ä½æ•° PnL:   ${median_pnl:>+.2f}/çª—å£")
    print(f"  æœ€å¤§ç›ˆåˆ©:     ${wr.pnl.max():>+.2f}")
    print(f"  æœ€å¤§äºæŸ:     ${wr.pnl.min():>+.2f}")

    # ç›ˆåˆ©/äºæŸçª—å£åˆ†åˆ«åˆ†æ
    wins = wr[wr.pnl > 0]
    losses = wr[wr.pnl <= 0]
    if len(wins) > 0 and len(losses) > 0:
        avg_win = wins.pnl.mean()
        avg_loss = losses.pnl.mean()
        profit_factor = wins.pnl.sum() / abs(losses.pnl.sum()) if losses.pnl.sum() != 0 else float('inf')
        print(f"  {'â”€' * 50}")
        print(f"  ç›ˆåˆ©çª—å£å‡å€¼: ${avg_win:>+.2f}  |  äºæŸçª—å£å‡å€¼: ${avg_loss:>+.2f}")
        print(f"  ç›ˆäºæ¯”:       {profit_factor:.2f}")

    # â”€â”€ ä¸ 0x1d çœŸå® PnL å¯¹æ¯” â”€â”€
    print(f"\n  â”€â”€ ä¸ 0x1d å®é™…å¯¹æ¯” â”€â”€")
    print(f"  0x1d æ€» PnL:  ${actual_total_pnl:>12,.2f}")
    print(f"  æ¨¡å‹æ€» PnL:   ${total_pnl:>12,.2f}")
    diff = total_pnl - actual_total_pnl
    print(f"  å·®å¼‚:         ${diff:>+12,.2f} ({'æ¨¡å‹æ›´ä¼˜' if diff > 0 else '0x1dæ›´ä¼˜'})")

    # â”€â”€ Top/Bottom çª—å£æ˜ç»† â”€â”€
    wr_sorted = wr.sort_values("pnl", ascending=False)
    print(f"\n  ç›ˆåˆ©æœ€å¤š (Top 5):")
    for _, row in wr_sorted.head(5).iterrows():
        short = row.slug.split("-")[-1] if "-" in str(row.slug) else str(row.slug)
        bias = "UP" if row.up_cost > row.dn_cost else "DOWN"
        print(f"    {short}: PnL=${row.pnl:>+.2f} ä¸‹æ³¨${row.total_cost:.0f} "
              f"æ–¹å‘={bias} èµ¢æ–¹={row.won} 0x1d=${row['0x1d_pnl']:>+.2f}")
    print(f"  äºæŸæœ€å¤š (Bottom 5):")
    for _, row in wr_sorted.tail(5).iterrows():
        short = row.slug.split("-")[-1] if "-" in str(row.slug) else str(row.slug)
        bias = "UP" if row.up_cost > row.dn_cost else "DOWN"
        print(f"    {short}: PnL=${row.pnl:>+.2f} ä¸‹æ³¨${row.total_cost:.0f} "
              f"æ–¹å‘={bias} èµ¢æ–¹={row.won} 0x1d=${row['0x1d_pnl']:>+.2f}")

    return {
        "total_pnl": total_pnl,
        "total_roi": total_roi,
        "win_rate": win_rate,
        "avg_pnl": avg_pnl,
        "n_windows": len(wr),
        "actual_pnl": actual_total_pnl,
        "window_df": wr,
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 7. ä¿å­˜æ¨¡å‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def save_model(model, eval_results: dict, threshold: float):
    """ä¿å­˜æ¨¡å‹ + é…ç½®, ç”¨äºç”Ÿäº§ç¯å¢ƒæ¨ç†"""
    model_path = MODEL_DIR / "signal_model.pkl"
    with open(model_path, "wb") as f:
        pickle.dump(model, f)

    features_used = eval_results.get("features_used", SIGNAL_FEATURES)
    model_label = eval_results.get("label", "å…¨ç‰¹å¾")

    config = {
        "features": list(features_used),
        "feature_groups": {
            "bn_momentum": BN_MOMENTUM,
            "bn_volatility": BN_VOLATILITY,
            "bn_advanced": BN_ADVANCED,
            "cl_momentum": CL_MOMENTUM,
            "cl_volatility": CL_VOLATILITY,
            "cl_advanced": CL_ADVANCED,
            "cross_source": CROSS_SOURCE,
            "pm_orderbook": PM_ORDERBOOK,
            "window_context": WINDOW_CONTEXT,
        },
        "threshold": threshold,
        "cv_accuracy": eval_results["cv_acc"],
        "cv_auc": eval_results["cv_auc"],
        "cv_f1": eval_results["cv_f1"],
        "n_features": len(features_used),
        "model_type": model_label,
        "pure_market": "æŒä»“" not in model_label,
        "training_label": eval_results.get("training_mode", "mimicry"),
        "excluded_features": EXCLUDED,
        "created_at": datetime.now(timezone.utc).isoformat(),
    }

    config_path = MODEL_DIR / "signal_config.json"
    with open(config_path, "w") as f:
        json.dump(config, f, indent=2, ensure_ascii=False)

    kb = model_path.stat().st_size / 1024
    print(f"\næ¨¡å‹å·²ä¿å­˜:")
    print(f"  {model_path} ({kb:.0f} KB)")
    print(f"  {config_path}")

    # æ¨ç†ç¤ºä¾‹
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ç”Ÿäº§æ¨ç†ä»£ç ç¤ºä¾‹                                                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                  â•‘
â•‘  import pickle, json, numpy as np                                â•‘
â•‘  model = pickle.load(open('signal_model.pkl', 'rb'))             â•‘
â•‘  cfg = json.load(open('signal_config.json'))                     â•‘
â•‘  threshold = cfg['threshold']                                    â•‘
â•‘                                                                  â•‘
â•‘  # ä»å®æ—¶æ•°æ®é‡‡é›† (BN WebSocket + CL RTDS + PM API)             â•‘
â•‘  features = collect_market_features()                            â•‘
â•‘  x = np.array([[features[f] for f in cfg['features']]])          â•‘
â•‘                                                                  â•‘
â•‘  prob_up = model.predict_proba(x)[0, 1]                          â•‘
â•‘  if prob_up > threshold:                                         â•‘
â•‘      signal, conf = 'UP', prob_up                                â•‘
â•‘  elif prob_up < (1 - threshold):                                 â•‘
â•‘      signal, conf = 'DOWN', 1 - prob_up                          â•‘
â•‘  else:                                                           â•‘
â•‘      signal, conf = 'HOLD', 0.5                                  â•‘
â•‘                                                                  â•‘
â•‘  print(f'Signal: {{signal}} (confidence: {{conf:.1%}})')         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 8. ä¸»å‡½æ•°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    # â”€â”€ å‚æ•°è§£æ â”€â”€
    report_only = "--report" in sys.argv
    rich_only = "--rich-only" in sys.argv
    pure_market = "--pure-market" in sys.argv
    compare_mode = "--compare" in sys.argv
    outcome_mode = "--outcome" in sys.argv
    threshold = 0.60
    for i, arg in enumerate(sys.argv):
        if arg == "--threshold" and i + 1 < len(sys.argv):
            threshold = float(sys.argv[i + 1])

    print("=" * 72)
    print("  0x1d å®æ—¶ä¿¡å·è’¸é¦ â€” Real-time Entry Signal Model")
    if outcome_mode:
        print("  ç›®æ ‡: é¢„æµ‹çª—å£ç»“ç®—èµ¢æ–¹, ä»¥ PnL ä¸ºæ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡")
    else:
        print("  ç›®æ ‡: å­¦ä¹  0x1d çš„å…¥åœºæ—¶æœºå’Œæ–¹å‘, å®æ—¶è¾“å‡º UP/DOWN/HOLD")
    print("=" * 72)
    if rich_only:
        print("  [æ¨¡å¼] ä»…ä½¿ç”¨æœ‰ ref_ts çš„é«˜è´¨é‡æ•°æ®")
    if pure_market:
        print("  [æ¨¡å¼] çº¯å¸‚åœºç‰¹å¾ (æ’é™¤æŒä»“/è¡Œä¸º/burst è‡ªç›¸å…³ç‰¹å¾)")
    if outcome_mode:
        print("  [æ¨¡å¼] ç»“ç®—æ ‡ç­¾ (è®­ç»ƒç›®æ ‡=çª—å£èµ¢æ–¹, è€Œéæ¨¡ä»¿0x1d)")
    if compare_mode:
        print("  [æ¨¡å¼] å¯¹æ¯”: å…¨ç‰¹å¾ vs çº¯å¸‚åœºç‰¹å¾")

    # â”€â”€ åŠ è½½æ•°æ® â”€â”€
    df = load_trades(rich_only=rich_only)

    # â”€â”€ åŠ è½½ç»“ç®—æ•°æ® (PnL è¯„ä¼° + outcome æ ‡ç­¾) â”€â”€
    settle = load_settlement_labels()

    # â”€â”€ çªå‘èšç±» â”€â”€
    df = deduplicate_bursts(df)

    # â”€â”€ ç‰¹å¾åˆ†æ â”€â”€
    feat_results = analyze_features(df)

    if report_only:
        print("\n[--report æ¨¡å¼, è·³è¿‡è®­ç»ƒ]")
        return

    # outcome æ¨¡å¼çš„æ ‡ç­¾
    outcome_labels = settle if outcome_mode else None

    # â”€â”€ å¯¹æ¯”æ¨¡å¼: åŒæ—¶è®­ç»ƒä¸¤ä¸ªæ¨¡å‹ â”€â”€
    if compare_mode:
        print("\n" + "#" * 72)
        print("#  å¯¹æ¯”æ¨¡å¼: å…¨ç‰¹å¾ vs çº¯å¸‚åœºç‰¹å¾")
        if outcome_mode:
            print("#  è®­ç»ƒæ ‡ç­¾: çª—å£ç»“ç®—ç»“æœ")
        print("#" * 72)

        m_all, r_all = train_signal_model(
            df, threshold=threshold,
            feature_set=SIGNAL_FEATURES, label="å…¨ç‰¹å¾(å«æŒä»“)",
            outcome_labels=outcome_labels,
        )
        m_pure, r_pure = train_signal_model(
            df, threshold=threshold,
            feature_set=PURE_MARKET_FEATURES, label="çº¯å¸‚åœºç‰¹å¾",
            outcome_labels=outcome_labels,
        )

        # å¯¹æ¯”æ±‡æ€»
        print("\n" + "=" * 72)
        print("  å¯¹æ¯”ç»“æœ: å…¨ç‰¹å¾ vs çº¯å¸‚åœºç‰¹å¾")
        print("=" * 72)
        print(f"{'æŒ‡æ ‡':<16} {'å…¨ç‰¹å¾(å«æŒä»“)':>16} {'çº¯å¸‚åœºç‰¹å¾':>16} {'å·®å¼‚':>10}")
        print("-" * 60)
        for metric, key in [("CV å‡†ç¡®ç‡", "cv_acc"), ("CV AUC", "cv_auc"), ("CV F1", "cv_f1")]:
            v_all = r_all[key]
            v_pure = r_pure[key]
            diff = v_pure - v_all
            print(f"  {metric:<14} {v_all:>16.4f} {v_pure:>16.4f} {diff:>+10.4f}")
        print(f"  {'ç‰¹å¾æ•°':<14} {len(SIGNAL_FEATURES):>16} {len(PURE_MARKET_FEATURES):>16}")
        print(f"  {'æœ€ä½³é˜ˆå€¼':<14} {r_all['best_threshold']:>16.2f} {r_pure['best_threshold']:>16.2f}")

        # Top ç‰¹å¾å¯¹æ¯”
        print(f"\n  å…¨ç‰¹å¾ Top-10:")
        for _, row in r_all["feature_importance"].head(10).iterrows():
            leaked = " âš  è‡ªç›¸å…³" if row["feature"] in POSITION_BEHAVIOR + BURST_AGGREGATE else ""
            print(f"    {row['feature']:<32} {row['importance']:>6.0f}{leaked}")
        print(f"\n  çº¯å¸‚åœº Top-10:")
        for _, row in r_pure["feature_importance"].head(10).iterrows():
            print(f"    {row['feature']:<32} {row['importance']:>6.0f}")

        # PnL æ¨¡æ‹Ÿå¯¹æ¯”
        print("\n" + "#" * 72)
        print("#  PnL æ¨¡æ‹Ÿå¯¹æ¯”")
        print("#" * 72)
        df_all = r_all.get("df_used", df)
        df_pure = r_pure.get("df_used", df)
        pnl_all = simulate_window_pnl(df_all, r_all["all_probs"], r_all["best_threshold"], settle, "fixed")
        pnl_pure = simulate_window_pnl(df_pure, r_pure["all_probs"], r_pure["best_threshold"], settle, "fixed")

        if pnl_all and pnl_pure:
            print(f"\n  â”€â”€ PnL å¯¹æ¯”æ€»ç»“ (fixed $10/ä¿¡å·) â”€â”€")
            print(f"  {'æŒ‡æ ‡':<16} {'å…¨ç‰¹å¾':>16} {'çº¯å¸‚åœº':>16}")
            print(f"  {'-' * 48}")
            print(f"  {'æ€» PnL':<14} ${pnl_all['total_pnl']:>+14,.2f} ${pnl_pure['total_pnl']:>+14,.2f}")
            print(f"  {'ROI':<14} {pnl_all['total_roi']:>+15.2%} {pnl_pure['total_roi']:>+15.2%}")
            print(f"  {'çª—å£èƒœç‡':<12} {pnl_all['win_rate']:>15.2%} {pnl_pure['win_rate']:>15.2%}")

        print(f"\n  ç»“è®º:")
        if r_pure["cv_acc"] >= r_all["cv_acc"] - 0.01:
            print(f"  âœ“ çº¯å¸‚åœºç‰¹å¾æ•ˆæœç›¸å½“æˆ–æ›´å¥½ â€” æŒä»“ç‰¹å¾æ˜¯å™ªå£°/è‡ªç›¸å…³, å¯å®‰å…¨ç§»é™¤")
        elif r_pure["cv_acc"] >= 0.52:
            print(f"  â— çº¯å¸‚åœºç‰¹å¾æœ‰è½»å¾®ä¸‹é™ä½†ä»æœ‰ä¿¡å· â€” å»ºè®®ä½¿ç”¨çº¯å¸‚åœºæ¨¡å‹(æ›´å¯é )")
        else:
            print(f"  âœ— çº¯å¸‚åœºç‰¹å¾æ•ˆæœæ˜¾è‘—ä¸‹é™ â€” å½“å‰æ•°æ®ä¸­å¸‚åœºç‰¹å¾åŒºåˆ†åº¦ä¸è¶³")
            print(f"    å»ºè®®: ç§¯ç´¯æ›´å¤šæ•°æ® / å¢åŠ æ–°ç‰¹å¾ç»´åº¦ (å¦‚ orderbook depth, funding rate)")

        # ä¿å­˜çº¯å¸‚åœºæ¨¡å‹ä¸ºä¸»æ¨¡å‹
        print(f"\n  ä¿å­˜çº¯å¸‚åœºæ¨¡å‹ä½œä¸ºç”Ÿäº§æ¨¡å‹...")
        save_model(m_pure, r_pure, r_pure["best_threshold"])
        return

    # â”€â”€ å¸¸è§„è®­ç»ƒ â”€â”€
    if pure_market or outcome_mode:
        features = PURE_MARKET_FEATURES
        label = "çº¯å¸‚åœºç‰¹å¾"
        if outcome_mode and not pure_market:
            print("  [è‡ªåŠ¨] --outcome æ¨¡å¼é»˜è®¤ä½¿ç”¨çº¯å¸‚åœºç‰¹å¾ (é¿å…æ³„æ¼)")
    else:
        features = SIGNAL_FEATURES
        label = "å…¨ç‰¹å¾(å«æŒä»“)"

    model, eval_results = train_signal_model(
        df, threshold=threshold, feature_set=features, label=label,
        outcome_labels=outcome_labels,
    )

    # â”€â”€ ä¿¡å·å›æµ‹ (æ–¹å‘å‡†ç¡®ç‡, è¾…åŠ©å‚è€ƒ) â”€â”€
    best_t = eval_results.get("best_threshold", threshold)
    df_used = eval_results.get("df_used", df)
    backtest_signals(df_used, eval_results["all_probs"], best_t)

    # â”€â”€ â­ PnL æ¨¡æ‹Ÿ (æ ¸å¿ƒè¯„ä¼°) â”€â”€
    pnl_fixed = simulate_window_pnl(df_used, eval_results["all_probs"], best_t, settle, "fixed")
    pnl_conf = simulate_window_pnl(df_used, eval_results["all_probs"], best_t, settle, "confidence")

    # â”€â”€ ä¿å­˜ â”€â”€
    save_model(model, eval_results, best_t)

    # â”€â”€ æ€»ç»“ â”€â”€
    print("\n" + "=" * 72)
    print("  è®­ç»ƒå®Œæˆ!")
    print(f"  æ¨¡å¼:      {label}")
    print(f"  è®­ç»ƒæ ‡ç­¾:  {'çª—å£ç»“ç®—ç»“æœ (outcome)' if outcome_mode else 'æ¨¡ä»¿0x1dæ–¹å‘ (mimicry)'}")
    print(f"  CV å‡†ç¡®ç‡: {eval_results['cv_acc']:.4f} (éšæœºåŸºå‡†: 0.5000)")
    print(f"  CV AUC:    {eval_results['cv_auc']:.4f}")
    print(f"  æœ€ä½³é˜ˆå€¼:  {best_t:.2f}")
    if pnl_fixed:
        print(f"  â”€â”€ PnL æ ¸å¿ƒæŒ‡æ ‡ (fixed $10/ç¬”) â”€â”€")
        print(f"  æ¨¡å‹ PnL:  ${pnl_fixed['total_pnl']:>+,.2f} (ROI: {pnl_fixed['total_roi']:>+.2%})")
        print(f"  çª—å£èƒœç‡:  {pnl_fixed['win_rate']:.2%}")
        print(f"  0x1d PnL:  ${pnl_fixed['actual_pnl']:>+,.2f}")
    if pnl_conf:
        print(f"  â”€â”€ PnL æ ¸å¿ƒæŒ‡æ ‡ (confidenceç¼©æ”¾) â”€â”€")
        print(f"  æ¨¡å‹ PnL:  ${pnl_conf['total_pnl']:>+,.2f} (ROI: {pnl_conf['total_roi']:>+.2%})")
        print(f"  çª—å£èƒœç‡:  {pnl_conf['win_rate']:.2%}")
    print(f"  {'â”€' * 50}")
    print(f"  ä¿¡å·é€»è¾‘:  P(UP) > {best_t:.2f} â†’ ä¹°UP")
    print(f"              P(UP) < {1-best_t:.2f} â†’ ä¹°DOWN")
    print(f"              å…¶ä½™ â†’ HOLD (ä¸äº¤æ˜“)")
    print("=" * 72)
    if not pure_market and not outcome_mode:
        print("\n  âš  æ³¨æ„: å½“å‰ä½¿ç”¨å…¨ç‰¹å¾(å«æŒä»“), Topç‰¹å¾å¯èƒ½æ˜¯è‡ªç›¸å…³è€Œéå¸‚åœºä¿¡å·")
        print("  å»ºè®®è¿è¡Œ: python scripts/distill_signal.py --outcome --pure-market")
    if not outcome_mode:
        print("\n  ğŸ’¡ æ¨è: ä½¿ç”¨ --outcome æ¨¡å¼, ä»¥çª—å£ç›ˆäºä¸ºè®­ç»ƒç›®æ ‡:")
        print("     python scripts/distill_signal.py --outcome --rich-only")
    print("\n  ä¸‹ä¸€æ­¥:")
    print("  1. è®© monitor_0x1d.py æŒç»­è¿è¡Œæ”¶é›†æ›´å¤šæ•°æ® (ç›®æ ‡: 7å¤©+)")
    print("  2. æ¨èè®­ç»ƒå‘½ä»¤: python scripts/distill_signal.py --outcome --rich-only")
    print("  3. å°†æ¨¡å‹é›†æˆåˆ°äº¤æ˜“æœºå™¨äºº, æ›¿ä»£æ‰‹å·¥è§„åˆ™")


if __name__ == "__main__":
    main()
